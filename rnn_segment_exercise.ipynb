{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import metrics\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn import *\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import helper functions for processing data\n",
    "- [data_creator.py](/edit/data/data_creator.py)\n",
    "- [utils.py](/edit/utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data.data_creator import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define RNN model function for word segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_segment(features, targets, mode, params):\n",
    "    seq_feature = features['seq_feature']\n",
    "    seq_length = features['seq_length']\n",
    "    \n",
    "    # Create a variable matrix in which each row represents an embedding vector for a given character.\n",
    "    with tf.variable_scope(\"emb\"):\n",
    "        embeddings = tf.get_variable(\"char_emb\", shape=[params[???], params[???]])\n",
    "    \n",
    "    # Convert sequence features to sequence embeddings \n",
    "    seq_emb = tf.nn.embedding_lookup(params=???, ids=???)\n",
    "    \n",
    "    # Flatten each sequence embedding\n",
    "    # [[1st char embedding], [2nd char embedding], [3rd char embedding]] => [flatten seq embedding]\n",
    "    batch_size = tf.shape(seq_feature)[0]\n",
    "    time_step = tf.shape(seq_feature)[1]\n",
    "    flat_seq_emb = tf.reshape(seq_emb, shape=[batch_size, time_step, (params['k'] + 1) * params['emb_size']])\n",
    "    \n",
    "    cell = rnn.LSTMCell(num_units=???)\n",
    "    if mode == ModeKeys.TRAIN:\n",
    "        # Create a cell with added input and output dropout with given probabilities.\n",
    "        cell = rnn.DropoutWrapper(cell, ???)\n",
    "        \n",
    "    # Project output from RNN cell into classes\n",
    "    projection_cell = rnn.OutputProjectionWrapper(cell, ???)\n",
    "    \n",
    "    # Create a recurrent neural network from the projection_cell\n",
    "    logits, _ = tf.nn.dynamic_rnn(???, dtype=tf.float32)\n",
    "    \n",
    "    # Training samples in each batch might have different lengths. \n",
    "    # We put them in the same matrix by padding each sample to have the same length as the longest sample. \n",
    "    # We need to create a mask tensor to avoid accounting for losses from the padding. \n",
    "    weight_mask = tf.to_float(tf.sequence_mask(seq_length))\n",
    "    loss = seq2seq.sequence_loss(???)\n",
    "    \n",
    "    train_op = layers.optimize_loss(\n",
    "        loss=loss,\n",
    "        global_step=tf.contrib.framework.get_global_step(),\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        optimizer=tf.train.AdamOptimizer,\n",
    "        clip_gradients=params['grad_clip'],\n",
    "        summaries=[\n",
    "            \"learning_rate\",\n",
    "            \"loss\",\n",
    "            \"gradients\",\n",
    "            \"gradient_norm\",\n",
    "        ])\n",
    "    \n",
    "    # Determine the predicted classes from logits\n",
    "    pred_classes = tf.to_int32(tf.argmax(???))\n",
    "    \n",
    "    # Convert classes to whether each sequence is the beginning of a word (class = 0 or 3)\n",
    "    pred_words = tf.logical_or(tf.equal(???), tf.equal(???))\n",
    "    target_words = tf.logical_or(tf.equal(???), tf.equal(???))\n",
    "    \n",
    "    # Compute precisions and recall\n",
    "    precision = metrics.streaming_precision(???)\n",
    "    recall = metrics.streaming_recall(???)\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\": pred_classes\n",
    "    }\n",
    "    eval_metric_ops = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "    \n",
    "    return learn.ModelFnOps(mode, predictions, loss, train_op, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_params = dict(num_class=len(CLASS_MAP), num_char=len(CHARS_MAP), emb_size=128, rnn_units=256,\n",
    "                        input_keep_prob=0.85, output_keep_prob=0.85, learning_rate=10e-4, grad_clip=1.0, k=2)\n",
    "    \n",
    "    rnn_model = learn.Estimator(model_fn=???\n",
    "                                , params=model_params\n",
    "                                , model_dir=\"model/_rnn_model\"\n",
    "                                , config=learn.RunConfig(save_checkpoints_secs=30,\n",
    "                                                         keep_checkpoint_max=2))\n",
    "\n",
    "    train_input_fn = data_provider(\"data/_tf_records_k2/train\", batch_size=128)\n",
    "    test_input_fn = data_provider(\"data/_tf_records_k2/test\", batch_size=512)\n",
    "\n",
    "    validation_monitor = monitors.ValidationMonitor(input_fn=test_input_fn,\n",
    "                                                    eval_steps=10,\n",
    "                                                    every_n_steps=500,\n",
    "                                                    name='validation')\n",
    "\n",
    "    # rnn_model.fit(input_fn=train_input_fn, steps=1, monitors=[validation_monitor])\n",
    "    # rnn_model.evaluate(input_fn=train_input_fn, steps=1)\n",
    "\n",
    "    text = \"\"\"นางกุหลาบขายกุหลาบจำหน่ายไม้ดอกไม้ประดับ\"\"\"\n",
    "    tudkum(text, rnn_model, model_params['k'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
